"""Sampling and adaptive self-consistency logic."""
from __future__ import annotations

import logging
import math
import time
from collections import Counter
from dataclasses import dataclass
from typing import Dict, List, Sequence

from casc_lite.backends.base import BaseBackend, BackendError

from .entropy import EntropyStats
from .utils import ExperimentConfig, extract_number, normalize_text

logger = logging.getLogger(__name__)


@dataclass
class SamplingResult:
    prompt: str
    responses: List[str]
    votes: Dict[str, int]
    chosen: str
    canonical_choice: str
    entropy: EntropyStats
    latency_seconds: float
    generated_tokens: float
    n_used: int


class AdaptiveSampler:
    """Adaptive sampler that switches the number of samples via entropy."""

    def __init__(
        self,
        backend: BaseBackend,
        config: ExperimentConfig,
        mode: str | None = None,
        n_fixed: int | None = None,
    ) -> None:
        self.backend = backend
        self.config = config
        self.mode = (mode or config.mode).lower()
        self.n_fixed = n_fixed or config.n_fixed
        self.candidates = sorted(set(config.n_candidates))
        if not self.candidates:
            raise ValueError("n_candidates must contain at least one value")
        logger.debug("AdaptiveSampler initialized with candidates=%s", self.candidates)

    def _candidate_triplet(self) -> tuple[int, int, int]:
        if len(self.candidates) >= 3:
            return self.candidates[0], self.candidates[len(self.candidates) // 2], self.candidates[-1]
        if len(self.candidates) == 2:
            low, high = self.candidates
            mid = math.ceil((low + high) / 2)
            return low, mid, high
        value = self.candidates[0]
        return value, value, value

    def _next_candidate(self, current_count: int) -> int | None:
        for candidate in self.candidates:
            if candidate > current_count:
                return candidate
        return None

    def decide_n(self, entropy_value: float) -> int:
        if self.mode == "fixed":
            if self.n_fixed is None:
                raise ValueError("Fixed mode requires n_fixed to be provided")
            return self.n_fixed

        low, mid, high = self._candidate_triplet()
        a = self.config.a
        b = self.config.b
        logger.debug(
            "Entropy %.4f compared against thresholds a=%.4f, b=%.4f with candidates %s",
            entropy_value,
            a,
            b,
            (low, mid, high),
        )
        if entropy_value <= a:
            return low
        if entropy_value <= b:
            return mid
        return high

    def _canonicalize(self, text: str) -> str:
        number = extract_number(text)
        if number is not None:
            return number
        return normalize_text(text)

    def run(self, prompt: str) -> SamplingResult:
        attempts = self.config.retry_on_error + 1
        last_error: Exception | None = None

        for attempt in range(attempts):
            try:
                window_k = max(1, min(self.config.K, self.config.entropy_window))
                entropy_stats = self.backend.prefix_entropy(
                    prompt=prompt,
                    K=window_k,
                    temperature=self.config.T,
                    top_p=self.config.top_p,
                    top_k=self.config.top_k,
                )
                entropy_value = entropy_stats.average_entropy
                if not math.isfinite(entropy_value):
                    entropy_value = 0.0
                n = self.decide_n(entropy_value)
                responses: list[str] = []
                canonical_answers: list[str] = []
                tokens: list[int] = []
                vote_counter: Counter[str] = Counter()
                latency = 0.0
                target = n

                while True:
                    needed = target - len(responses)
                    if needed > 0:
                        start = time.perf_counter()
                        generations = self.backend.generate_n(
                            prompt=prompt,
                            n=needed,
                            temperature=self.config.T,
                            top_p=self.config.top_p,
                            top_k=self.config.top_k,
                            max_new_tokens=self.config.max_new_tokens,
                            min_new_tokens=self.config.min_new_tokens,
                        )
                        latency += time.perf_counter() - start
                        if not generations:
                            raise BackendError("No responses generated by backend")

                        for sample in generations:
                            text = str(sample.get("text", ""))
                            responses.append(text)
                            tokens.append(int(sample.get("num_generated_tokens", 0)))
                            canonical = self._canonicalize(text)
                            canonical_answers.append(canonical)
                            vote_counter[canonical] += 1

                    canonical_choice, top_votes = max(
                        vote_counter.items(),
                        key=lambda item: (item[1], -canonical_answers.index(item[0])),
                    )

                    majority = math.ceil(len(responses) / 2)
                    if top_votes >= majority:
                        break

                    next_target = self._next_candidate(len(responses))
                    if next_target is None or next_target == target:
                        break
                    logger.debug(
                        "Consensus %.4f insufficient with %d samples (top votes=%d); escalating to %d",
                        entropy_stats.average_entropy,
                        len(responses),
                        top_votes,
                        next_target,
                    )
                    target = next_target

                if not vote_counter:
                    raise BackendError("No responses generated by backend")

                avg_tokens = float(sum(tokens) / max(len(tokens), 1))
                chosen_raw = next(
                    resp
                    for resp, canonical in zip(responses, canonical_answers)
                    if canonical == canonical_choice
                )
                logger.info(
                    "Prompt processed with entropy %.4f -> n=%d, latency=%.3fs, avg_tokens=%.1f",
                    entropy_stats.average_entropy,
                    len(responses),
                    latency,
                    avg_tokens,
                )
                return SamplingResult(
                    prompt=prompt,
                    responses=responses,
                    votes=dict(vote_counter),
                    chosen=chosen_raw.strip(),
                    canonical_choice=canonical_choice,
                    entropy=entropy_stats,
                    latency_seconds=latency,
                    generated_tokens=avg_tokens,
                    n_used=len(responses),
                )
            except Exception as exc:  # pylint: disable=broad-except
                last_error = exc
                logger.warning(
                    "AdaptiveSampler attempt %d/%d failed: %s",
                    attempt + 1,
                    attempts,
                    exc,
                )
                if attempt + 1 >= attempts:
                    break
                time.sleep(0.1 * (attempt + 1))

        assert last_error is not None
        raise BackendError(f"Failed to generate after {attempts} attempts: {last_error}")
